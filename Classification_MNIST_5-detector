from sklearn.datasets import fetch_openml # We can get MNIST dataset (70,000) from OpenML or Kaggle
import matplotlib.pyplot as plt
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import precision_score, recall_score


mnist = fetch_openml('mnist_784', as_frame=False) # MNIST dataset contains images, and DataFrames aren’t ideal for that, so it’s preferable to set as_frame=False
X,y = mnist.data, mnist.target
# There are 70,000 images, and each image has 784 features
X
y
def plot_digit(image_data):
    image = image_data.reshape(28,28)
    plt.imshow(image,cmap = 'binary')
    plt.axis('off')
some_digit = X[0]
plot_digit(some_digit)
plt.show()

y[0]

# fetch_openml() is actually already split into a training set (the first 60,000 images) and a test set (the last 10,000 images)
X_train, X_test, y_train, y_test = X[:60000],X[60000:],y[:60000],y[60000:] # All cross-validation folds will be similar
y_train_5 = (y_train == '5')
y_test_5 = (y_test == '5')
sgd_clf.predict([some_digit])

cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
>>> array([0.95035, 0.96035, 0.9604 ]) # Maybe is overfit

# Skewed datasets-5s only has 10%
# Confusion matrix

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
y_train_pred
cm = confusion_matrix(y_train_5, y_train_pred)
cm
>>> array([[53892,   687],
       [ 1891,  3530]], dtype=int64)
# True Negative: 53892   false positive: 687

precision_score(y_train_5, y_train_pred)
recall_score(y_train_5, y_train_pred)
